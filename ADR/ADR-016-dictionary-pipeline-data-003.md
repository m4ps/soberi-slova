# ADR-016: CSV pipeline словаря и O(1) индекс нормализованных слов

- Статус: accepted
- Дата: 2026-02-25
- Связанные задачи: [DATA]-[003]

## Контекст

Этап DATA-003 требует перевести `data/dictionary.csv` в runtime-индекс словаря, пригодный для доменной валидации слов и дальнейшей генерации уровней.

PRD/TECHSPEC фиксируют обязательные правила:
- только lower-case кириллица;
- `ё` не эквивалентна `е`;
- только существительные (`type=noun`);
- невалидные строки словаря должны игнорироваться;
- lookup должен быть O(1) по normalized слову;
- статистика отбракованных строк должна быть доступна для telemetry/log.

## Решение

1. Добавить модуль `src/domain/WordValidation/dictionary-pipeline.ts` с API:
   - `buildDictionaryIndexFromCsv(csvContent)`;
   - `normalizeDictionaryWord(word)`;
   - `isValidNormalizedDictionaryWord(word)`.
2. В качестве in-memory индекса использовать `Map<string, WordEntry>` и `Set<string>` по `normalized` ключу.
3. Фильтровать CSV-строки по правилам:
   - обязательные колонки `id,bare,rank,type`;
   - `type === noun`;
   - `bare` уже в lower-case виде;
   - `bare` соответствует `^[а-яё]+$`;
   - `id` — безопасный целый неотрицательный;
   - `rank` — конечное число.
4. Дубликаты по `normalized` отбрасывать (оставлять первое валидное вхождение) для детерминированности индекса.
5. Возвращать подробную статистику reject-причин (`rejectedByReason`) как прямой telemetry/log payload.

## Последствия

- Словарь получает детерминированный и быстрый lookup-контур для `WordValidation` и следующих задач генератора (`CODE-001`).
- Потенциальные проблемы качества входного CSV становятся наблюдаемыми через reject-метрики.
- Правило "оставлять первое валидное вхождение при дублях" фиксирует стабильное поведение при обновлениях словаря и исключает неоднозначность ранга/entry для одного `normalized` ключа.
